<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Translator Settings</title>
    <style>
      body{font-family:Inter,Segoe UI,Arial;padding:18px;background:#071025;color:#e6eef6}
      .hint{font-size:12px;color:#7b8793;margin-top:6px}
      button{margin-top:14px;padding:10px;border-radius:8px;border:0;background:#4f46e5;color:#fff}
      pre{background:#0b1220;padding:10px;border-radius:8px;color:#e6eef6;white-space:pre-wrap}
      /* small copy button style */
      .small-btn{padding:6px 8px;border-radius:6px;font-size:13px;margin-left:8px}
      .copy-feedback{display:inline-block;margin-left:10px;color:#9aa4b2;font-size:13px}
    </style>
  </head>
  <body>
    <h2>Settings</h2>

    <div class="hint">
      This extension uses only a local translation service â€” no Google or external APIs.
      Ensure your local llama-server is running and accessible at http://127.0.0.1:8080.
    </div>

    <!-- status area will be created/managed by options.js -->
    <div id="statusContainer" style="margin-top:12px"></div>

    <div style="margin-top:16px">
      <label style="display:block;margin-bottom:8px;color:#9aa4b2">Run llama-server command (example):</label>
      <pre id="runCmd">llama-server -m "C:\path\to\N-ATLaS-GGUF-Q8_0.gguf" --port 8080</pre>
      <button id="copyCmd" class="small-btn">Copy</button>
      <span id="copyFeedback" class="copy-feedback" aria-live="polite"></span>

      <div style="margin-top:12px;color:#9aa4b2">
        <strong>Quick local setup (Windows)</strong>
        <ol style="margin-top:8px">
          <li>Install a llama.cpp/llama-server build (example via <code>winget</code>):
            <pre style="margin:6px 0">winget install llama.cpp</pre>
          </li>
          <li>Download the N-ATLaS GGUF model and save it in a folder called <code>llama</code> on your Desktop (example filename):
            <pre style="margin:6px 0">N-ATLaS-GGUF-Q8_0.gguf</pre>
          </li>
          <li>Open a Command Prompt in that folder and confirm the file exists:
            <pre style="margin:6px 0">dir
You should see: N-ATLaS-GGUF-Q8_0.gguf</pre>
          </li>
          <li>Start the model server (from the same folder):
            <pre style="margin:6px 0">llama-server -m N-ATLaS-GGUF-Q8_0.gguf --port 8080</pre>
          </li>
          <li>Test the OpenAI-style API (open a new Command Prompt and run):
            <pre style="margin:6px 0">curl http://127.0.0.1:8080/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\": \"local\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}"
            </pre>
            You should see a response like:
            <pre style="margin:6px 0">{
  "choices": [{
    "message": { "content": "Hello! How can I assist you?" }
  }]
}</pre>
          </li>
        </ol>
        <div style="margin-top:8px">Or specify the full path when starting the server:
          <pre style="margin:6px 0">llama-server -m "C:\Users\YourUser\Desktop\llama\N-ATLaS-GGUF-Q8_0.gguf" --port 8080</pre>
        </div>
      </div>
    </div>

    <script src="options.js" defer></script>
  </body>
</html>